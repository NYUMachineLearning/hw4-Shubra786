---
title: 'Machine Learning 2019: Feature Selection'
author: "Sonali Narang"
date: "October 24, 2019"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Feature Selection 

In machine learning, feature selection is the process of choosing variables that are useful in predicting the response variable. Selecting the right features in your data can mean the difference between mediocre performance with long training times and great performance with short training times that are less computationally intensive. 

Often, data can contain attributes that are highly correlated with each other or not useful in helping predict our response variable. Many methods perform better if such variables are removed. Feature selection is usually imporant to implement during the data pre-processing steps of machine learning. 

```{r load relevant libraries, include=FALSE}
library(tidyverse)
library(caret)
library(randomForest)
library(mlbench)
library(glmnet)
```

## The Breast Cancer Dataset
699 Observations, 11 variables
Predictor Variable: Class- benign or malignant 

```{r load Breast Cancer dataset}
data(BreastCancer)
head(BreastCancer)
dim(BreastCancer)
summary(BreastCancer$Class)
```

## Feature Selection Using Filter Methods: Pearson's Correlation 

Filter Methods are generally used as a preprocessing step so the selection of features is independednt of any machine learning algorithms. Features are selected on the basis of their scores in various statistical tests for their correlation with the outcome variable. 

Below we will identify attributes that are highly correlated using Pearson's correlation which is a measure for quantifying linear dependence between X and Y. Ranges between -1 and 1. 

```{r correlation}
BreastCancer_num = transform(BreastCancer, Id = as.numeric(Id), 
                         Cl.thickness = as.numeric(Cl.thickness),
                         Cell.size = as.numeric(Cell.size),
                         Cell.shape = as.numeric(Cell.shape), 
                         Marg.adhesion = as.numeric(Marg.adhesion),
                         Epith.c.size = as.numeric(Epith.c.size),
                         Bare.nuclei = as.numeric(Bare.nuclei), 
                         Bl.cromatin = as.numeric(Bl.cromatin), 
                         Normal.nucleoli = as.numeric(Normal.nucleoli),
                         Mitoses = as.numeric(Mitoses))

BreastCancer_num[is.na(BreastCancer_num)] = 0

#calculate correlation matrix using pearson correlation (others include spearman and kendall)
correlation_matrix = cor(BreastCancer_num[,1:10])

#visualize correlation matrix
library(corrplot)
corrplot(correlation_matrix, order = "hclust")

#apply correlation filter of 0.7
highly_correlated <- colnames(BreastCancer[, -1])[findCorrelation(correlation_matrix, cutoff = 0.7, verbose = TRUE)]

#which features are highly correlated and can be removed
highly_correlated
```
## Feature Selection Using Wrapper Methods: Recursive Feature Elimination (RFE)

Wrapper methods are a bit more computationally intensive since we will select features based on a specific machine learning algorith. 

The RFE function implements backwards selection of predictors based on predictor importance ranking. The predictors are ranked and the less important ones are sequentially eliminated prior to modeling. The goal is to find a subset of predictors that can be used to produce an accurate model.

```{r RFE}
data(BreastCancer)
BreastCancer_num = transform(BreastCancer, Id = as.numeric(Id), 
                         Cl.thickness = as.numeric(Cl.thickness),
                         Cell.size = as.numeric(Cell.size),
                         Cell.shape = as.numeric(Cell.shape), 
                         Marg.adhesion = as.numeric(Marg.adhesion),
                         Epith.c.size = as.numeric(Epith.c.size),
                         Bare.nuclei = as.numeric(Bare.nuclei), 
                         Bl.cromatin = as.numeric(Bl.cromatin), 
                         Normal.nucleoli = as.numeric(Normal.nucleoli),
                         Mitoses = as.numeric(Mitoses))

BreastCancer_num[is.na(BreastCancer_num)] = 0

#define the control 
control = rfeControl(functions = caretFuncs, number = 2)

# run the RFE algorithm
results = rfe(BreastCancer_num[,1:10], BreastCancer_num[,11], sizes = c(2,5,9), rfeControl = control, method = "svmRadial")

results
results$variables
```

## Feature Selection Using Embedded Methods: Lasso

Least Absolute Shrinkage and Selection Operator (LASSO) regression


```{r Lasso}
set.seed(24)

#convert data
x = x <- as.matrix(BreastCancer_num[,1:10])
y = as.double(as.matrix(ifelse(BreastCancer_num[,11]=='benign', 0, 1))) 

#fit Lasso model 
cv.lasso <- cv.glmnet(x, y, family='binomial', alpha=1, parallel=TRUE, standardize=TRUE, type.measure='auc')

plot(cv.lasso)

cat('Min Lambda: ', cv.lasso$lambda.min, '\n 1Sd Lambda: ', cv.lasso$lambda.1se)
df_coef <- round(as.matrix(coef(cv.lasso, s=cv.lasso$lambda.min)), 2)

# See all contributing variables
df_coef[df_coef[, 1] != 0, ]
```

## Feature Selection Using Embedded Methods: RandomForest
Random Forest Importance function and caret package's varImp functions perform similarly.

```{r importance}
#data
data(BreastCancer)
train_size <- floor(0.75 * nrow(BreastCancer))
set.seed(24)
train_pos <- sample(seq_len(nrow(BreastCancer)), size = train_size)

#convert to numeric
BreastCancer_num = transform(BreastCancer, Id = as.numeric(Id), 
                         Cl.thickness = as.numeric(Cl.thickness),
                         Cell.size = as.numeric(Cell.size),
                         Cell.shape = as.numeric(Cell.shape), 
                         Marg.adhesion = as.numeric(Marg.adhesion),
                         Epith.c.size = as.numeric(Epith.c.size),
                         Bare.nuclei = as.numeric(Bare.nuclei), 
                         Bl.cromatin = as.numeric(Bl.cromatin), 
                         Normal.nucleoli = as.numeric(Normal.nucleoli),
                         Mitoses = as.numeric(Mitoses))

BreastCancer_num[is.na(BreastCancer_num)] = 0

train_classification <- BreastCancer_num[train_pos, ]
test_classification <- BreastCancer_num[-train_pos, ]

#fit a model
rfmodel = randomForest(Class ~ Id + Cl.thickness + Cell.size + Cell.shape + Marg.adhesion + Epith.c.size + Bare.nuclei + Bl.cromatin + Normal.nucleoli +  Mitoses, data=train_classification,  importance = TRUE, oob.times = 15, confusion = TRUE)

#rank features based on importance 
importance(rfmodel)

```



## Homework

1. Compare the most important features from at least 2 different classes of feature selection methods covered in this tutorial with any reasonable machine learning dataset from mlbench. Do these feature selection methods provide similar results? 

```{r correlation}

# FEATURE SELECTION BASED ON CORRELATION (FILTER METHOD)

BreastCancer_num = transform(BreastCancer, Id = as.numeric(Id), 
                         Cl.thickness = as.numeric(Cl.thickness),
                         Cell.size = as.numeric(Cell.size),
                         Cell.shape = as.numeric(Cell.shape), 
                         Marg.adhesion = as.numeric(Marg.adhesion),
                         Epith.c.size = as.numeric(Epith.c.size),
                         Bare.nuclei = as.numeric(Bare.nuclei), 
                         Bl.cromatin = as.numeric(Bl.cromatin), 
                         Normal.nucleoli = as.numeric(Normal.nucleoli),
                         Mitoses = as.numeric(Mitoses))

BreastCancer_num[is.na(BreastCancer_num)] = 0

# ensure results are reproducible
set.seed(7)

#calculate correlation matrix using pearson correlation (others include spearman and kendall)
correlation_matrix = cor(BreastCancer_num[,1:10])

#visualize correlation matrix
library(corrplot)
corrplot(correlation_matrix, order = "hclust")

#apply correlation filter of 0.7
highly_correlated <- colnames(BreastCancer[, -1])[findCorrelation(correlation_matrix, cutoff = 0.7, verbose = TRUE)]

#which features are highly correlated and can be removed
highly_correlated

# OUTCOME

# Cell.shape AND Marg.adhesion ARE BOTH DETERMINED TO BE HIGHLY CORRELATED. THIS MEANS THEY WILL NOT BE RELATIVELY USEFUL IN DEMONSTRATING THE VARIANCE IN THE DATA. ON THAT BASIS, USING THIS METHOD OF FEATURE SELECTION, THEY ARE NOT SELECTED. 

```



```{r RFE}

# RFE FEATURE SELECTION (WRAPPER METHOD)

# ensure results are reproducible
set.seed(123)

# define the control using a random forest selection function (PURPOSEFULLY DIFFERENT FROM TUTORIAL, FOR THE SAKE OF EXPERIMENTATION)
control = rfeControl(functions=rfFuncs, method="cv", number=10) # NOTE 10 FOLD CROSS- VALIDATION RESAMPLING METHOD

# run the RFE algorithm (NOTE: ALL FEATURES WILL BE TESTED AT THE SAME TIME)
results = rfe(BreastCancer_num[,1:10], BreastCancer_num[,11], sizes = c(1:10), rfeControl = control)

results
results$variables

# plot the results (NOTE: ADDITIONAL STEP INCORPORATED, AS VISUALS FACILITATE DEMONSTRATION OF RESULTS)
plot(results, type=c("g", "o"))

# OUTCOME

# THE PLOT CLEARLY SHOWS THAT OPTIMAL FEATURE SELECTION INVOLVES USING 8 FEATURES. IF Id IS DISCOUNTED, Mitoses IS THE FEATURE EXCLUDED USING THE RFE METHOD cf. Cell.shape AND Marg.adhesion FROM THE CORRELATION METHOD.

```

2. Attempt a feature selection method not covered in this tutorial (backward elimination, forward propogation, etc.)

```{r}

# FEATURE SELECTION USING THE CARET PACKAGE'S varImp FUNCTION (EMBEDDED METHOD)

# ensure results are repeatable
set.seed(7)

# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)

# train the model
model <- train(Class~., data=BreastCancer_num, method="lvq", preProcess="scale", trControl=control) # NOTE: Learning Vector Quantization (LVQ) model constructed

# determine feature (variable) importance
importance <- varImp(model, scale=FALSE)

importance

# plot importance
plot(importance)

# OUTCOME

# THE PLOT VERY CLEARLY SHOWS FEATURES RANKED IN ORDER OF IMPORTANCE. IF Id IS DISCOUNTED, Mitoses WOULD BE THE MOST LIKELY FEATURE EXCLUDED USING THIS METHOD. THE IMPORTANCE METHOD IS COMPARABLE TO THE RFE METHOD IN ITS CHOICE OF FEATURE EXCLUSION, ALTHOUGH THE RANKING OF FEATURES IS NOT QUITE THE SAME.

```





